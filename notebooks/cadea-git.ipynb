{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "00151168-9355-41e1-969a-fac646fa3884",
    "_uuid": "10aaf0f3-09da-4597-a928-a4b7de345d0e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-12T08:56:46.784631Z",
     "iopub.status.busy": "2026-02-12T08:56:46.784306Z",
     "iopub.status.idle": "2026-02-12T08:56:47.087400Z",
     "shell.execute_reply": "2026-02-12T08:56:47.086493Z",
     "shell.execute_reply.started": "2026-02-12T08:56:46.784603Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T08:56:49.631333Z",
     "iopub.status.busy": "2026-02-12T08:56:49.630880Z",
     "iopub.status.idle": "2026-02-12T08:56:50.490061Z",
     "shell.execute_reply": "2026-02-12T08:56:50.489038Z",
     "shell.execute_reply.started": "2026-02-12T08:56:49.631304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cadea'...\n",
      "remote: Enumerating objects: 49, done.\u001b[K\n",
      "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
      "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
      "remote: Total 49 (delta 9), reused 46 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (49/49), 3.67 MiB | 19.38 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mehreen019/cadea.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T08:56:53.346991Z",
     "iopub.status.busy": "2026-02-12T08:56:53.346250Z",
     "iopub.status.idle": "2026-02-12T08:56:53.352559Z",
     "shell.execute_reply": "2026-02-12T08:56:53.351862Z",
     "shell.execute_reply.started": "2026-02-12T08:56:53.346952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/cadea\n"
     ]
    }
   ],
   "source": [
    "%cd cadea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T08:56:55.694297Z",
     "iopub.status.busy": "2026-02-12T08:56:55.693966Z",
     "iopub.status.idle": "2026-02-12T08:57:10.969480Z",
     "shell.execute_reply": "2026-02-12T08:57:10.968766Z",
     "shell.execute_reply.started": "2026-02-12T08:56:55.694270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.46.3 (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.11.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.2.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (26.0rc2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (2.32.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.3->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (0.70.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 7)) (5.9.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3->-r requirements.txt (line 1)) (1.2.1rc0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3->-r requirements.txt (line 1)) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.22.0)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "Successfully installed tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T08:57:10.971594Z",
     "iopub.status.busy": "2026-02-12T08:57:10.971292Z",
     "iopub.status.idle": "2026-02-12T08:57:10.976006Z",
     "shell.execute_reply": "2026-02-12T08:57:10.975083Z",
     "shell.execute_reply.started": "2026-02-12T08:57:10.971558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T08:57:10.977310Z",
     "iopub.status.busy": "2026-02-12T08:57:10.977074Z",
     "iopub.status.idle": "2026-02-12T09:05:13.998577Z",
     "shell.execute_reply": "2026-02-12T09:05:13.997797Z",
     "shell.execute_reply.started": "2026-02-12T08:57:10.977288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ± Random seed set to: 42\n",
      "ğŸ§ª Using TEST configuration (5-minute run)\n",
      "âš ï¸  Checkpointing DISABLED (running in no-checkpoint mode)\n",
      "\n",
      "================================================================================\n",
      "CADEA Training - Local Machine\n",
      "================================================================================\n",
      "Device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.64 GB\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Checkpoint directory: /kaggle/working/cadea/checkpoints\n",
      "Loading tokenizer...\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54.5k/54.5k [00:00<00:00, 5.92MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.09M/9.09M [00:00<00:00, 27.4MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296/296 [00:00<00:00, 2.57MB/s]\n",
      "Loading model...\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:00<00:00, 3.97MB/s]\n",
      "2026-02-12 08:57:26.281587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770886646.465544     125 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770886646.518724     125 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770886646.961741     125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770886646.961775     125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770886646.961779     125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770886646.961782     125 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.47G/2.47G [00:08<00:00, 305MB/s]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:00<00:00, 1.33MB/s]\n",
      "âœ“ Gradient checkpointing enabled\n",
      "âœ“ Model loaded: 1.24B params\n",
      "VRAM: 1.50 GB allocated, 1.50 GB reserved\n",
      "\n",
      "================================================================================\n",
      "PREPARING DATASETS\n",
      "================================================================================\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "README.md: 7.47kB [00:00, 3.61MB/s]\n",
      "data/train-00000-of-00001-a09b74b3ef9c3b(â€¦): 100%|â–ˆ| 24.2M/24.2M [00:00<00:00, 5\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 52002/52002 [00:00<00:00, 290432.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:11<00:00,  1.14s/ examples]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 111.89 examples/s]\n",
      "âœ“ Dataset prepared: 10 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "README.md: 22.5kB [00:00, 53.6MB/s]\n",
      "Bangla_INST_400K.csv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 867M/867M [00:03<00:00, 258MB/s]\n",
      "Generating train split: 100%|â–ˆ| 342391/342391 [00:17<00:00, 19172.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 7744.86 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 14286.75 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 755.45 examples/s]\n",
      "âœ“ Dataset prepared: 50 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "README.md: 5.97kB [00:00, 17.3MB/s]\n",
      "data/train-00000-of-00001-b2881e1b9f14c3(â€¦): 100%|â–ˆ| 3.55M/3.55M [00:00<00:00, 5\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 10000/10000 [00:00<00:00, 366135.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 8426.36 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 19164.32 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2453.44 examples/s]\n",
      "âœ“ Dataset prepared: 50 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 9060.93 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2980.39 examples/s]\n",
      "âœ“ Dataset prepared: 50 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 14974.10 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 62119.43 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 832.79 examples/s]\n",
      "âœ“ Dataset prepared: 500 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 17970.61 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 92381.48 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 3465.97 examples/s]\n",
      "âœ“ Dataset prepared: 500 samples\n",
      "â­ï¸  Skipping checkpoint detection (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "STAGE 1: ENGLISH TRAINING\n",
      "================================================================================\n",
      "English:   0%|                                           | 0/10 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Step 0, VRAM: 4.52GB\n",
      "Step 0 | VRAM: 4.52 GB allocated, 4.61 GB reserved\n",
      "English: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.40it/s]\n",
      "âœ“ English training complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION AFTER ENGLISH TRAINING\n",
      "================================================================================\n",
      "EN Test  - Loss: 1.7880, Perplexity: 5.98                                       \n",
      "EN Train - Loss: 1.5924, Perplexity: 4.92\n",
      "â­ï¸  Skipping checkpoint save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "COMPUTING ENGLISH BASELINE GRADIENT PROFILE\n",
      "================================================================================\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.03it/s]\n",
      "\n",
      "âœ“ English baseline computed\n",
      "  Average loss: 1.7388\n",
      "â­ï¸  Skipping baseline gradient save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2: BENGALI TRAINING WITH CONFLICT MONITORING (vs English baseline)\n",
      "================================================================================\n",
      "Training on Bengali for 100 steps (logging every 100)...\n",
      "Bengali training: 100%|â–ˆ| 100/100 [00:56<00:00,  1.77it/s, loss=0.8889, peak_layStep 100 | VRAM: 6.01 GB allocated, 10.09 GB reserved\n",
      "Bengali training: 100%|â–ˆ| 100/100 [00:56<00:00,  1.77it/s, loss=0.8889, peak_lay\n",
      "\n",
      "âœ“ Bengali training complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION AFTER BENGALI TRAINING\n",
      "================================================================================\n",
      "EN Test  - Loss: 1.7742, Perplexity: 5.90                                       \n",
      "EN Train - Loss: 1.5837, Perplexity: 4.87\n",
      "BN Test  - Loss: 0.9204, Perplexity: 2.51\n",
      "BN Train - Loss: 0.9185, Perplexity: 2.51\n",
      "\n",
      "âš ï¸  EN Train Forgetting: -0.9% (TRUE catastrophic forgetting)\n",
      "âš ï¸  EN Test Degradation: -1.4% (generalization loss)\n",
      "â­ï¸  Skipping checkpoint save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "COMPUTING BENGALI BASELINE GRADIENT PROFILE (post-training)\n",
      "================================================================================\n",
      "Bengali baseline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.84it/s]\n",
      "\n",
      "âœ“ Bengali baseline computed\n",
      "  Average loss: 0.9185\n",
      "â­ï¸  Skipping baseline gradient save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "STAGE 3: ARABIC TRAINING WITH CONFLICT MONITORING (vs Bengali baseline)\n",
      "================================================================================\n",
      "Training on Arabic for 100 steps (logging every 100)...\n",
      "Arabic training: 100%|â–ˆ| 100/100 [00:59<00:00,  1.72it/s, loss=1.6365, peak_layeStep 100 | VRAM: 7.10 GB allocated, 11.18 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training: 100%|â–ˆ| 100/100 [01:00<00:00,  1.66it/s, loss=1.6365, peak_laye\n",
      "\n",
      "âœ“ Arabic training complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION AFTER ARABIC TRAINING\n",
      "================================================================================\n",
      "EN Test  - Loss: 1.7667, Perplexity: 5.85                                       \n",
      "EN Train - Loss: 1.5831, Perplexity: 4.87\n",
      "BN Test  - Loss: 0.8979, Perplexity: 2.45\n",
      "BN Train - Loss: 0.8962, Perplexity: 2.45\n",
      "AR Test  - Loss: 2.8613, Perplexity: 17.48\n",
      "AR Train - Loss: 2.6553, Perplexity: 14.23\n",
      "â­ï¸  Skipping checkpoint save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "GENERATING COMBINED ANALYSIS VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Combined visualization saved: results/combined_en_bn_ar_analysis.png\n",
      "\n",
      "================================================================================\n",
      "VALIDATION RESULTS - SEQUENTIAL TRANSFER ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STAGE 1: ENâ†’BN VALIDATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š KEY FINDINGS (ENâ†’BN):\n",
      "  â”œâ”€ Data quality: 100% (0 checkpoints skipped due to OOM)\n",
      "  â”œâ”€ Peak conflict migrated 0 times across 1 checkpoints\n",
      "  â”œâ”€ 1 unique layers experienced peak conflict\n",
      "  â”œâ”€ Initial peak: Layer 9\n",
      "  â””â”€ Final peak: Layer 9\n",
      "\n",
      "ğŸ“ˆ MOST NON-STATIONARY LAYERS (ENâ†’BN):\n",
      "  â”œâ”€ Layer 0: variance = 0.0000\n",
      "  â”œâ”€ Layer 3: variance = 0.0000\n",
      "  â”œâ”€ Layer 6: variance = 0.0000\n",
      "\n",
      "ğŸ¯ HYPOTHESIS VALIDATION (ENâ†’BN):\n",
      "  âš ï¸  WEAK: Limited peak migration detected (0 transitions)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2: BNâ†’AR VALIDATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š KEY FINDINGS (BNâ†’AR):\n",
      "  â”œâ”€ Data quality: 100% (0 checkpoints skipped due to OOM)\n",
      "  â”œâ”€ Peak conflict migrated 0 times across 1 checkpoints\n",
      "  â”œâ”€ 1 unique layers experienced peak conflict\n",
      "  â”œâ”€ Initial peak: Layer 12\n",
      "  â””â”€ Final peak: Layer 12\n",
      "\n",
      "ğŸ“ˆ MOST NON-STATIONARY LAYERS (BNâ†’AR):\n",
      "  â”œâ”€ Layer 0: variance = 0.0000\n",
      "  â”œâ”€ Layer 3: variance = 0.0000\n",
      "  â”œâ”€ Layer 6: variance = 0.0000\n",
      "\n",
      "ğŸ¯ HYPOTHESIS VALIDATION (BNâ†’AR):\n",
      "  âš ï¸  WEAK: Limited peak migration detected (0 transitions)\n",
      "\n",
      "================================================================================\n",
      "ğŸ† OVERALL HYPOTHESIS VALIDATION\n",
      "================================================================================\n",
      "âŒ HYPOTHESIS WEAK: Both stages show relatively stationary conflicts\n",
      "   â†’ Consider longer training, different language pairs, or architecture\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ CATASTROPHIC FORGETTING ANALYSIS\n",
      "================================================================================\n",
      "Note: TRUE forgetting = degradation on TRAINING data (not just test data)\n",
      "\n",
      "ğŸ“‰ ENGLISH FORGETTING (ENâ†’BN):\n",
      "  Train: 4.92 â†’ 4.87 (-0.9%) [TRUE FORGETTING]\n",
      "  Test:  5.98 â†’ 5.90 (-1.4%) [generalization]\n",
      "  After AR Train: 4.87 (-0.9% total)\n",
      "  After AR Test:  5.85 (-2.1% total)\n",
      "\n",
      "ğŸ“‰ BENGALI FORGETTING (BNâ†’AR):\n",
      "  Train: 2.51 â†’ 2.45 (-2.2%) [TRUE FORGETTING]\n",
      "  Test:  2.51 â†’ 2.45 (-2.2%) [generalization]\n",
      "\n",
      "ğŸ¯ BACKWARD TRANSFER (BWT) SUMMARY:\n",
      "  Average TRAIN forgetting: -1.6% (TRUE catastrophic forgetting)\n",
      "  Average TEST degradation: -2.2% (generalization loss)\n",
      "\n",
      "âœ… MINIMAL FORGETTING (<5% train degradation)\n",
      "   â†’ Model retains previous knowledge well\n",
      "\n",
      "================================================================================\n",
      "EXPORTING RESULTS\n",
      "================================================================================\n",
      "âœ“ Results exported to: results/cadea_sequential_transfer_data.json\n",
      "âœ“ Performance tracking saved to: results/performance_tracking.json\n",
      "\n",
      "ğŸ“Š VRAM Summary:\n",
      "  Peak allocated: 7.10 GB\n",
      "  Peak reserved: 11.18 GB\n",
      "\n",
      "ğŸ‰ Training complete! Checkpoints in: ./checkpoints\n",
      "   Results in: ./results\n"
     ]
    }
   ],
   "source": [
    "!python scripts/cadea_local_training.py --checkpointing no --test --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T09:09:26.088395Z",
     "iopub.status.busy": "2026-02-12T09:09:26.087433Z",
     "iopub.status.idle": "2026-02-12T10:25:45.785787Z",
     "shell.execute_reply": "2026-02-12T10:25:45.784857Z",
     "shell.execute_reply.started": "2026-02-12T09:09:26.088356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ± Random seed set to: 42\n",
      "ğŸš€ Using FULL configuration\n",
      "ğŸ“ Overriding EN samples: 1000\n",
      "ğŸ“ Overriding BN samples: 1500\n",
      "ğŸ“ Overriding AR samples: 1500\n",
      "ğŸ“ Overriding batch size: 2\n",
      "âš ï¸  Checkpointing DISABLED (running in no-checkpoint mode)\n",
      "\n",
      "================================================================================\n",
      "CADEA Training - Local Machine\n",
      "================================================================================\n",
      "Device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.64 GB\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Checkpoint directory: /kaggle/working/cadea/checkpoints\n",
      "Loading tokenizer...\n",
      "Loading model...\n",
      "2026-02-12 09:09:32.677934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770887372.699103     291 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770887372.706001     291 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770887372.724236     291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770887372.724263     291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770887372.724267     291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770887372.724270     291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "âœ“ Gradient checkpointing enabled\n",
      "âœ“ Model loaded: 1.24B params\n",
      "VRAM: 1.50 GB allocated, 1.50 GB reserved\n",
      "\n",
      "================================================================================\n",
      "PREPARING DATASETS\n",
      "================================================================================\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 18061.15 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5565.69 examples/s]\n",
      "âœ“ Dataset prepared: 1000 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 15143.28 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 69494.28 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:01<00:00, 833.60 examples/s]\n",
      "âœ“ Dataset prepared: 1500 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 19960.33 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 163968.10 examples/s]\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 3466.83 examples/s]\n",
      "âœ“ Dataset prepared: 1500 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "âœ“ Dataset prepared: 50 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'md-nishat-008/Bangla-Instruct' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "âœ“ Dataset prepared: 500 samples\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'arbml/CIDAR' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "âœ“ Dataset prepared: 500 samples\n",
      "â­ï¸  Skipping checkpoint detection (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "STAGE 1: ENGLISH TRAINING\n",
      "================================================================================\n",
      "English:   0%|                                          | 0/500 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Step 0, VRAM: 4.52GB\n",
      "Step 0 | VRAM: 4.52 GB allocated, 4.67 GB reserved\n",
      "English:   2%|â–‹                                | 10/500 [00:11<08:43,  1.07s/it]Step 10, VRAM: 4.52GB\n",
      "English:   4%|â–ˆâ–                               | 20/500 [00:22<08:38,  1.08s/it]Step 20, VRAM: 4.52GB\n",
      "English:   6%|â–ˆâ–‰                               | 30/500 [00:33<08:28,  1.08s/it]Step 30, VRAM: 4.52GB\n",
      "English:   8%|â–ˆâ–ˆâ–‹                              | 40/500 [00:44<08:24,  1.10s/it]Step 40, VRAM: 4.52GB\n",
      "English:  10%|â–ˆâ–ˆâ–ˆâ–                             | 50/500 [00:55<08:22,  1.12s/it]Step 50, VRAM: 4.52GB\n",
      "English:  12%|â–ˆâ–ˆâ–ˆâ–‰                             | 60/500 [01:06<08:17,  1.13s/it]Step 60, VRAM: 4.52GB\n",
      "English:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 70/500 [01:18<08:12,  1.14s/it]Step 70, VRAM: 4.52GB\n",
      "English:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 80/500 [01:30<08:13,  1.17s/it]Step 80, VRAM: 4.52GB\n",
      "English:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 90/500 [01:41<08:06,  1.19s/it]Step 90, VRAM: 4.52GB\n",
      "English:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 100/500 [01:54<07:59,  1.20s/it]Step 100, VRAM: 4.52GB\n",
      "Step 100 | VRAM: 4.52 GB allocated, 4.67 GB reserved\n",
      "English:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 110/500 [02:06<07:42,  1.19s/it]Step 110, VRAM: 4.52GB\n",
      "English:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 120/500 [02:17<07:23,  1.17s/it]Step 120, VRAM: 4.52GB\n",
      "English:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 130/500 [02:29<07:12,  1.17s/it]Step 130, VRAM: 4.52GB\n",
      "English:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 140/500 [02:41<07:00,  1.17s/it]Step 140, VRAM: 4.52GB\n",
      "English:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 150/500 [02:53<06:50,  1.17s/it]Step 150, VRAM: 4.52GB\n",
      "English:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 160/500 [03:05<06:42,  1.18s/it]Step 160, VRAM: 4.52GB\n",
      "English:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 170/500 [03:17<06:33,  1.19s/it]Step 170, VRAM: 4.52GB\n",
      "English:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 180/500 [03:29<06:19,  1.19s/it]Step 180, VRAM: 4.52GB\n",
      "English:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 190/500 [03:40<06:04,  1.17s/it]Step 190, VRAM: 4.52GB\n",
      "English:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 200/500 [03:52<05:50,  1.17s/it]Step 200, VRAM: 4.52GB\n",
      "Step 200 | VRAM: 4.52 GB allocated, 4.67 GB reserved\n",
      "English:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 210/500 [04:04<05:38,  1.17s/it]Step 210, VRAM: 4.52GB\n",
      "English:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 220/500 [04:16<05:28,  1.17s/it]Step 220, VRAM: 4.52GB\n",
      "English:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 230/500 [04:28<05:17,  1.18s/it]Step 230, VRAM: 4.52GB\n",
      "English:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 240/500 [04:40<05:07,  1.18s/it]Step 240, VRAM: 4.52GB\n",
      "English:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 250/500 [04:52<04:53,  1.18s/it]Step 250, VRAM: 4.52GB\n",
      "English:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 260/500 [05:04<04:43,  1.18s/it]Step 260, VRAM: 4.52GB\n",
      "English:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 270/500 [05:15<04:30,  1.18s/it]Step 270, VRAM: 4.52GB\n",
      "English:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 280/500 [05:27<04:18,  1.18s/it]Step 280, VRAM: 4.52GB\n",
      "English:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 290/500 [05:39<04:07,  1.18s/it]Step 290, VRAM: 4.52GB\n",
      "English:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 300/500 [05:51<03:54,  1.17s/it]Step 300, VRAM: 4.52GB\n",
      "Step 300 | VRAM: 4.52 GB allocated, 4.67 GB reserved\n",
      "English:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 310/500 [06:03<03:42,  1.17s/it]Step 310, VRAM: 4.52GB\n",
      "English:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 320/500 [06:15<03:31,  1.17s/it]Step 320, VRAM: 4.52GB\n",
      "English:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 330/500 [06:27<03:20,  1.18s/it]Step 330, VRAM: 4.52GB\n",
      "English:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 340/500 [06:39<03:09,  1.19s/it]Step 340, VRAM: 4.52GB\n",
      "English:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 350/500 [06:51<02:57,  1.18s/it]Step 350, VRAM: 4.52GB\n",
      "English:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 360/500 [07:02<02:45,  1.18s/it]Step 360, VRAM: 4.52GB\n",
      "English:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 370/500 [07:14<02:32,  1.17s/it]Step 370, VRAM: 4.52GB\n",
      "English:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 380/500 [07:26<02:20,  1.17s/it]Step 380, VRAM: 4.52GB\n",
      "English:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 390/500 [07:38<02:08,  1.17s/it]Step 390, VRAM: 4.52GB\n",
      "English:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 400/500 [07:50<01:57,  1.18s/it]Step 400, VRAM: 4.52GB\n",
      "Step 400 | VRAM: 4.52 GB allocated, 4.67 GB reserved\n",
      "English:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 410/500 [08:02<01:46,  1.18s/it]Step 410, VRAM: 4.52GB\n",
      "English:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 420/500 [08:14<01:34,  1.18s/it]Step 420, VRAM: 4.52GB\n",
      "English:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 430/500 [08:26<01:23,  1.19s/it]Step 430, VRAM: 4.52GB\n",
      "English:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 440/500 [08:38<01:10,  1.17s/it]Step 440, VRAM: 4.52GB\n",
      "English:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 450/500 [08:49<00:58,  1.18s/it]Step 450, VRAM: 4.52GB\n",
      "English:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 460/500 [09:01<00:46,  1.17s/it]Step 460, VRAM: 4.52GB\n",
      "English:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/500 [09:13<00:35,  1.18s/it]Step 470, VRAM: 4.52GB\n",
      "English:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 480/500 [09:25<00:23,  1.18s/it]Step 480, VRAM: 4.52GB\n",
      "English:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 490/500 [09:37<00:11,  1.18s/it]Step 490, VRAM: 4.52GB\n",
      "English: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [09:49<00:00,  1.18s/it]\n",
      "âœ“ English training complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION AFTER ENGLISH TRAINING\n",
      "================================================================================\n",
      "EN Test  - Loss: 1.3938, Perplexity: 4.03                                       \n",
      "EN Train - Loss: 1.4094, Perplexity: 4.09\n",
      "â­ï¸  Skipping checkpoint save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "COMPUTING ENGLISH BASELINE GRADIENT PROFILE\n",
      "================================================================================\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:44<00:00,  1.05s/it]\n",
      "\n",
      "âœ“ English baseline computed\n",
      "  Average loss: 1.3723\n",
      "â­ï¸  Skipping baseline gradient save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2: BENGALI TRAINING WITH CONFLICT MONITORING (vs English baseline)\n",
      "================================================================================\n",
      "Training on Bengali for 1500 steps (logging every 100)...\n",
      "Bengali training:   7%| | 100/1500 [01:53<25:45,  1.10s/it, loss=0.9348, peak_laStep 100 | VRAM: 6.04 GB allocated, 10.12 GB reserved\n",
      "Bengali training:  13%|â–| 200/1500 [03:46<23:55,  1.10s/it, loss=84.8363, peak_lStep 200 | VRAM: 6.04 GB allocated, 9.65 GB reserved\n",
      "Bengali training:  20%|â–| 300/1500 [05:39<22:01,  1.10s/it, loss=79.0137, peak_lStep 300 | VRAM: 6.04 GB allocated, 9.65 GB reserved\n",
      "Bengali training:  27%|â–| 400/1500 [07:31<20:08,  1.10s/it, loss=70.0938, peak_lStep 400 | VRAM: 6.04 GB allocated, 10.70 GB reserved\n",
      "Bengali training:  33%|â–| 500/1500 [09:24<18:27,  1.11s/it, loss=67.9287, peak_lStep 500 | VRAM: 6.04 GB allocated, 10.70 GB reserved\n",
      "Bengali training:  40%|â–| 600/1500 [11:17<16:30,  1.10s/it, loss=65.4116, peak_lStep 600 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training:  47%|â–| 700/1500 [13:10<14:43,  1.10s/it, loss=65.6347, peak_lStep 700 | VRAM: 6.04 GB allocated, 10.70 GB reserved\n",
      "Bengali training:  53%|â–Œ| 800/1500 [15:03<12:54,  1.11s/it, loss=64.5501, peak_lStep 800 | VRAM: 6.04 GB allocated, 10.70 GB reserved\n",
      "Bengali training:  60%|â–Œ| 900/1500 [16:56<11:02,  1.10s/it, loss=64.7795, peak_lStep 900 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training:  67%|â–‹| 1000/1500 [18:49<09:11,  1.10s/it, loss=62.4174, peak_Step 1000 | VRAM: 6.04 GB allocated, 9.65 GB reserved\n",
      "Bengali training:  73%|â–‹| 1100/1500 [20:42<07:21,  1.10s/it, loss=61.4931, peak_Step 1100 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training:  80%|â–Š| 1200/1500 [22:35<05:31,  1.10s/it, loss=60.5790, peak_Step 1200 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training:  87%|â–Š| 1300/1500 [24:28<03:40,  1.10s/it, loss=63.2749, peak_Step 1300 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training:  93%|â–‰| 1400/1500 [26:21<01:50,  1.10s/it, loss=60.7132, peak_Step 1400 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training: 100%|â–ˆ| 1500/1500 [28:14<00:00,  1.10s/it, loss=59.8296, peak_Step 1500 | VRAM: 6.04 GB allocated, 10.18 GB reserved\n",
      "Bengali training: 100%|â–ˆ| 1500/1500 [28:14<00:00,  1.13s/it, loss=59.8296, peak_\n",
      "\n",
      "âœ“ Bengali training complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION AFTER BENGALI TRAINING\n",
      "================================================================================\n",
      "EN Test  - Loss: 1.5764, Perplexity: 4.84                                       \n",
      "EN Train - Loss: 1.6629, Perplexity: 5.27\n",
      "BN Test  - Loss: 0.5906, Perplexity: 1.80\n",
      "BN Train - Loss: 0.5896, Perplexity: 1.80\n",
      "\n",
      "âš ï¸  EN Train Forgetting: +28.9% (TRUE catastrophic forgetting)\n",
      "âš ï¸  EN Test Degradation: +20.0% (generalization loss)\n",
      "â­ï¸  Skipping checkpoint save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "COMPUTING BENGALI BASELINE GRADIENT PROFILE (post-training)\n",
      "================================================================================\n",
      "Bengali baseline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:46<00:00,  1.07s/it]\n",
      "\n",
      "âœ“ Bengali baseline computed\n",
      "  Average loss: 0.5976\n",
      "â­ï¸  Skipping baseline gradient save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "STAGE 3: ARABIC TRAINING WITH CONFLICT MONITORING (vs Bengali baseline)\n",
      "================================================================================\n",
      "Training on Arabic for 1500 steps (logging every 100)...\n",
      "Arabic training:   7%| | 100/1500 [01:52<25:31,  1.09s/it, loss=2.0038, peak_layStep 100 | VRAM: 7.14 GB allocated, 11.21 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  13%|â–| 200/1500 [03:44<23:43,  1.09s/it, loss=282.2884, peak_lStep 200 | VRAM: 7.14 GB allocated, 10.76 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  20%|â–| 300/1500 [05:36<22:02,  1.10s/it, loss=267.4163, peak_lStep 300 | VRAM: 7.14 GB allocated, 11.29 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  27%|â–| 400/1500 [07:28<20:04,  1.09s/it, loss=270.3345, peak_lStep 400 | VRAM: 7.14 GB allocated, 11.82 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  33%|â–| 500/1500 [09:20<18:20,  1.10s/it, loss=257.8839, peak_lStep 500 | VRAM: 7.14 GB allocated, 10.76 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  40%|â–| 600/1500 [11:12<16:26,  1.10s/it, loss=251.4793, peak_lStep 600 | VRAM: 7.14 GB allocated, 11.82 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  47%|â–| 700/1500 [13:04<14:40,  1.10s/it, loss=251.2991, peak_lStep 700 | VRAM: 7.14 GB allocated, 11.29 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  53%|â–Œ| 800/1500 [14:57<12:48,  1.10s/it, loss=252.3472, peak_lStep 800 | VRAM: 7.14 GB allocated, 11.29 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  60%|â–Œ| 900/1500 [16:49<10:55,  1.09s/it, loss=251.0555, peak_lStep 900 | VRAM: 7.14 GB allocated, 10.76 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  67%|â–‹| 1000/1500 [18:41<09:06,  1.09s/it, loss=242.1854, peak_Step 1000 | VRAM: 7.14 GB allocated, 11.42 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  73%|â–‹| 1100/1500 [20:33<07:14,  1.09s/it, loss=243.1402, peak_Step 1100 | VRAM: 7.14 GB allocated, 11.79 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  80%|â–Š| 1200/1500 [22:25<05:30,  1.10s/it, loss=246.4441, peak_Step 1200 | VRAM: 7.14 GB allocated, 11.23 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  87%|â–Š| 1300/1500 [24:17<03:38,  1.09s/it, loss=250.1763, peak_Step 1300 | VRAM: 7.14 GB allocated, 11.23 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training:  93%|â–‰| 1400/1500 [26:09<01:49,  1.10s/it, loss=239.5157, peak_Step 1400 | VRAM: 7.14 GB allocated, 11.23 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training: 100%|â–ˆ| 1500/1500 [28:01<00:00,  1.09s/it, loss=245.4500, peak_Step 1500 | VRAM: 7.14 GB allocated, 11.23 GB reserved\n",
      "âš ï¸  WARNING: VRAM usage high (>7 GB), may OOM soon!\n",
      "Arabic training: 100%|â–ˆ| 1500/1500 [28:02<00:00,  1.12s/it, loss=245.4500, peak_\n",
      "\n",
      "âœ“ Arabic training complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION AFTER ARABIC TRAINING\n",
      "================================================================================\n",
      "EN Test  - Loss: 1.5105, Perplexity: 4.53                                       \n",
      "EN Train - Loss: 1.5844, Perplexity: 4.88\n",
      "BN Test  - Loss: 0.6824, Perplexity: 1.98\n",
      "BN Train - Loss: 0.6817, Perplexity: 1.98\n",
      "AR Test  - Loss: 2.3837, Perplexity: 10.85\n",
      "AR Train - Loss: 2.3368, Perplexity: 10.35\n",
      "â­ï¸  Skipping checkpoint save (checkpointing disabled)\n",
      "\n",
      "================================================================================\n",
      "GENERATING COMBINED ANALYSIS VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Combined visualization saved: results/combined_en_bn_ar_analysis.png\n",
      "\n",
      "================================================================================\n",
      "VALIDATION RESULTS - SEQUENTIAL TRANSFER ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STAGE 1: ENâ†’BN VALIDATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š KEY FINDINGS (ENâ†’BN):\n",
      "  â”œâ”€ Data quality: 100% (0 checkpoints skipped due to OOM)\n",
      "  â”œâ”€ Peak conflict migrated 10 times across 15 checkpoints\n",
      "  â”œâ”€ 5 unique layers experienced peak conflict\n",
      "  â”œâ”€ Initial peak: Layer 6\n",
      "  â””â”€ Final peak: Layer 3\n",
      "\n",
      "ğŸ“ˆ MOST NON-STATIONARY LAYERS (ENâ†’BN):\n",
      "  â”œâ”€ Layer 3: variance = 0.0021\n",
      "  â”œâ”€ Layer 6: variance = 0.0020\n",
      "  â”œâ”€ Layer 0: variance = 0.0019\n",
      "\n",
      "ğŸ¯ HYPOTHESIS VALIDATION (ENâ†’BN):\n",
      "  âœ… MODERATE: Peak conflict oscillated across 10 layer transitions\n",
      "  âœ… Conflict variance highly non-uniform (ratio: 10.66x)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2: BNâ†’AR VALIDATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š KEY FINDINGS (BNâ†’AR):\n",
      "  â”œâ”€ Data quality: 100% (0 checkpoints skipped due to OOM)\n",
      "  â”œâ”€ Peak conflict migrated 12 times across 15 checkpoints\n",
      "  â”œâ”€ 6 unique layers experienced peak conflict\n",
      "  â”œâ”€ Initial peak: Layer 0\n",
      "  â””â”€ Final peak: Layer 9\n",
      "\n",
      "ğŸ“ˆ MOST NON-STATIONARY LAYERS (BNâ†’AR):\n",
      "  â”œâ”€ Layer 6: variance = 0.0015\n",
      "  â”œâ”€ Layer 0: variance = 0.0014\n",
      "  â”œâ”€ Layer 3: variance = 0.0014\n",
      "\n",
      "ğŸ¯ HYPOTHESIS VALIDATION (BNâ†’AR):\n",
      "  âœ… STRONG: Peak conflict migrated 9 layers (shallowâ†’deep or vice versa)\n",
      "  âœ… Conflict variance highly non-uniform (ratio: 22.51x)\n",
      "\n",
      "================================================================================\n",
      "ğŸ† OVERALL HYPOTHESIS VALIDATION\n",
      "================================================================================\n",
      "âœ… STRONGLY VALIDATED: Both ENâ†’BN and BNâ†’AR show NON-STATIONARY conflicts\n",
      "   â†’ Static expert allocation is SUBOPTIMAL across sequential transfer\n",
      "   â†’ CADEA paper premise is SOUND for multilingual scenarios\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¬ CATASTROPHIC FORGETTING ANALYSIS\n",
      "================================================================================\n",
      "Note: TRUE forgetting = degradation on TRAINING data (not just test data)\n",
      "\n",
      "ğŸ“‰ ENGLISH FORGETTING (ENâ†’BN):\n",
      "  Train: 4.09 â†’ 5.27 (+28.9%) [TRUE FORGETTING]\n",
      "  Test:  4.03 â†’ 4.84 (+20.0%) [generalization]\n",
      "  After AR Train: 4.88 (+19.1% total)\n",
      "  After AR Test:  4.53 (+12.4% total)\n",
      "\n",
      "ğŸ“‰ BENGALI FORGETTING (BNâ†’AR):\n",
      "  Train: 1.80 â†’ 1.98 (+9.7%) [TRUE FORGETTING]\n",
      "  Test:  1.80 â†’ 1.98 (+9.6%) [generalization]\n",
      "\n",
      "ğŸ¯ BACKWARD TRANSFER (BWT) SUMMARY:\n",
      "  Average TRAIN forgetting: 14.4% (TRUE catastrophic forgetting)\n",
      "  Average TEST degradation: 11.0% (generalization loss)\n",
      "\n",
      "âœ… CATASTROPHIC FORGETTING CONFIRMED (>10% train degradation)\n",
      "   â†’ Model is overwriting previously learned knowledge\n",
      "   â†’ This validates the need for dynamic expert allocation\n",
      "\n",
      "================================================================================\n",
      "EXPORTING RESULTS\n",
      "================================================================================\n",
      "âœ“ Results exported to: results/cadea_sequential_transfer_data.json\n",
      "âœ“ Performance tracking saved to: results/performance_tracking.json\n",
      "\n",
      "ğŸ“Š VRAM Summary:\n",
      "  Peak allocated: 7.14 GB\n",
      "  Peak reserved: 11.82 GB\n",
      "\n",
      "ğŸ‰ Training complete! Checkpoints in: ./checkpoints\n",
      "   Results in: ./results\n"
     ]
    }
   ],
   "source": [
    "!python scripts/cadea_local_training.py --checkpointing no --batch-size 2 --en-samples 1000 --bn-samples 1500 --ar-samples 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
